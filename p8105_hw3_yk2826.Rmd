---
title: "Homework 3"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(patchwork)

knitr::opts_chunk$set(
  fig.width = 12,
  fig.height = 10, 
  out.width = "90%"
)
theme_set(theme_bw() + theme(legend.position = "bottom"))
```

## Problem 1
load and clean the BRFSS data:
```{r clean_BRFSS}
library(p8105.datasets)
data(brfss_smart2010)

brfss_data = 
  brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  rename(state = locationabbr, location = locationdesc) %>% 
  filter(topic == "Overall Health") %>% 
  mutate(response = factor(response, levels = 
                             c("Excellent", "Very good", "Good", "Fair", "Poor")))
```
Answer the following questions:

a.In 2002, which states were observed at 7 locations?
```{r count_location}
brfss_data %>% 
  group_by(year, state) %>%
  summarize(n_location = n_distinct(location)) %>% 
  filter(year == 2002, n_location == 7)
```
From the result, we can see in 2002, CT, FL and NC were observed at 7 locations.

b.Make a “spaghetti plot” that shows the number of locations in each state from 2002 to 2010.
```{r spaghetti_plot}
brfss_data %>% 
  group_by(year, state) %>%
  summarize(n_location = n_distinct(location)) %>% 
  ggplot(aes(x = year, y = n_location, color = state)) + 
    geom_line() + 
    labs(
      title = "Number of locations plot",
      x = "Year",
      y = "The number of locations in each state"
    ) +
  scale_color_hue(
    name = "State",
    h = c(100, 350),
    l = 75
  ) +
  theme(legend.position = "right")
```

I used "geom_line()" to make the "spaghetti plot" to show the trends in the number of locations across 50 states and the District of Columbia over years. The x axis shows the year from 2002 to 2010, and the y axis shows the number of locations in each state in each year. States are differentiated by different colors. The number of locations did not change much over years in most states except Florida. Florida had more than 40 locations in year 2007 and 2010, while the number of locations in all states in other years were less than 20.

c.Make a table showing, for the years 2002, 2006, and 2010, the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State.
```{r excellent_table}
brfss_data %>% 
  group_by(year) %>%
  filter(year %in% c(2002,2006,2010),
         state == "NY", response == "Excellent") %>% 
  summarize(mean_excellent = mean(data_value, na.rm = TRUE),
            sd_excellent = sd(data_value, na.rm = TRUE)) %>% 
  knitr::kable(digits = 1)
```

The table shows the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State for the years 2002, 2006, and 2010. We can see, the proportion of “Excellent” responses was highest in 2002, then slightly declined from 2002 to 2006, and were approximately equal in 2006 and 2010. The standard deviation of the proportion of “Excellent” responses across locations in NY State declined from 2002 to 2010, which means there were less variability with respect to the proportion of “Excellent” responses in years 2006 and 2010 than in year 2002 maybe because the number of observations increased.

d.For each year and state, compute the average proportion in each response category (taking the average across locations in a state). Make a five-panel plot that shows, for each response category separately, the distribution of these state-level averages over time.
```{r response_plot}
response_average = 
  brfss_data %>% 
  group_by(year, state, response) %>%
  summarize(average = mean(data_value, na.rm = TRUE)) 
response_average

response_average %>% 
  ggplot(aes(x = year, y = average, color = state)) + 
     geom_line() + 
    facet_grid(~response) + 
    labs(
      title = "State-level averages vs year",
      x = "Year",
      y = "State-level averages")
```

I used line plots because they are the most appropriate to show the trends of distribution of the average proportion in each state over time. We can see from the plots that for all states the state-level average proportions in "Very good" were the highest while the state-level average proportions in "Poor" were the lowest from 2002 to 2010. Besides, the distributions of these state-level averages for each category remains relatively constant over years, which implies that responses did not vary apparently over time.

## Problem 2
```{r load_instacart}
data(instacart)
head(instacart)
```
The "instacart" dataset gives information about orders from the online company Instacart. The dataset contains `r nrow(instacart)` observations of `r n_distinct(instacart$user_id)` unique users, where each row in the dataset is a product from an order. There is a single order per user in this dataset.

There are 15 variables in this dataset:

order_id: order identifier  
product_id: product identifier  
add_to_cart_order: order in which each product was added to cart  
reordered: 1 if this prodcut has been ordered by this user in the past, 0 otherwise  
user_id: customer identifier  
eval_set: which evaluation set this order belongs in   
order_number: the order sequence number for this user (1 = first, n = nth)  
order_dow: the day of the week on which the order was placed  
order_hour_of_day: the hour of the day on which the order was placed  
days_since_prior_order: days since the last order, capped at 30, NA if order_number = 1  
product_name: name of the product  
aisle_id: aisle identifier  
department_id: department identifier  
aisle: the name of the aisle  
department: the name of the department

Here's an illstrative example of observations:  
The first row means that the user whose ID is 112108 ordered Bulgarian Yogurt on Thursday at 10 am in his/her fourth order and Bulgarian Yogurt was the first item he/she added to cart in this order. This prodcut has been ordered by this user in the past. There were 9 days since his/her last order. The Id of Bulgarian Yogurt is 49302. It was on yogurt aisle and the aisle's id is 120. The Bulgarian Yogurt was from dary eggs department and the Id of this department was 16. This order's Id was 1 and the order belongs in "train" evaluation set.

Answer the following questions:

a.How many aisles are there, and which aisles are the most items ordered from?
```{r count_aisles}
n_item = 
  instacart %>% 
  group_by(aisle) %>% 
  summarise(n_item = n()) %>% 
  arrange(desc(n_item))
head(n_item)
```
There are `r nrow(n_item)` aisles. "Fresh vegetables" and "fresh fruits" are ordered most with 150609 and 150473 ordered items respectively.

b.Make a plot that shows the number of items ordered in each aisle. Order aisles sensibly, and organize your plot so others can read it.
```{r chunk_set, include=FALSE}
knitr::opts_chunk$set(
  fig.width = 15,
  fig.height = 20, 
  out.width = "90%"
)
```

```{r aisle_plot}
instacart %>% 
  group_by(aisle) %>% 
  summarise(n_item = n()) %>% 
  ggplot(aes(x = reorder(aisle, desc(n_item)), y = n_item)) +
    geom_col(fill = "blue") +
    labs(
      title = "Number of items ordered in each aisle",
      x = "Aisles",
      y = "Number of items ordered") +
  coord_flip() +
  theme(legend.position = "none") +
  theme(axis.text = element_text(size = 12))
```

There are 134 aisles in the dataset. This bar plot shows the number of items ordered across all aisles, ordered from most to least. Fresh vegetables and fresh fruits are most items ordered from while there are least items ordered from the beauty aisle, with only 287 ordered items. Because there are too many aisles, I made a horizontal instead of a vertical bar plot to make the name of the aisles more readilable.

c.Make a table showing the most popular item in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”.
```{r popular_item}
instacart %>% 
  filter(aisle %in% c("baking ingredients", 
                      "dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle, product_name) %>% 
  summarise(n = n()) %>% 
  filter(min_rank(desc(n)) == 1) %>% 
  rename(Aisle = aisle, 
         "Product Name" = product_name,
         "Number of orders" = n) %>% 
  knitr::kable(digits = 1)
```

The above table shows the most popular items in the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”. Light Brown Sugar is the most frequently ordered item in the “baking ingredients” aisle with 499 orders. The most popular item is Snack Sticks Chicken & Rice Recipe Dog Treats in the “dog food care” aisle with 30 order and the most popular item in "packaged vegetables fruits" aisle is Organic Baby Spinach with 9784 orders.

d.Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week.
```{r order_hour}
instacart %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>% 
  summarise(mean_order_hour = mean(order_hour_of_day, na.rm = TRUE)) %>% 
  spread(key = order_dow, value = mean_order_hour) %>% 
  rename("Product Name" = product_name, 
         Sunday = "0", Monday = "1", Tuesday = "2",
         Wednesday = "3", Thursday = "4", Friday = "5", 
         Saturday = "6") %>% 
  knitr::kable(digits = 1)
```

The above table shows that Coffee Ice Cream is usually order in the early afternoon at around 2pm to 3pm for every day of the week except for Friday when the ice cream is ordered earlier. Customers order pink lady apples, on average, ealiear than order Coffee Ice Cream, mostly around noon between 11am to 2pm for every day of the week except Wednesday.

## Problem 3
load ny_noaa data
```{r load_noaa}
data(ny_noaa)
head(ny_noaa)

ny_noaa %>% 
  filter(is.na(prcp) | is.na(snow) | is.na(snwd) | is.na(tmax) | is.na(tmin)) %>% 
  nrow()
ny_noaa %>% 
  filter(is.na(prcp) & is.na(snow) & is.na(snwd) & is.na(tmax) & is.na(tmin)) %>% 
  nrow()
NA_prcp = 
  ny_noaa %>% filter(is.na(prcp)) %>% nrow()
NA_prcp / nrow(ny_noaa)
NA_tmax = ny_noaa %>% filter(is.na(tmax)) %>% nrow()
NA_tmax / nrow(ny_noaa)
NA_tmin = ny_noaa %>% filter(is.na(tmin)) %>% nrow()
NA_tmin / nrow(ny_noaa)
NA_snow = ny_noaa %>% filter(is.na(snow)) %>% nrow()
NA_snow / nrow(ny_noaa)
NA_snwd = ny_noaa %>% filter(is.na(snwd)) %>% nrow()
NA_snwd / nrow(ny_noaa)
```

The "NOAA"ny_noaa" dataset contains weather information of all New York state weather stations from January 1, 1981 through December 31, 2010. The dataset has `r nrow(ny_noaa)` observations and 7 variables. Each observation contains weather data collected from one of the weather stations on one day.  
The meaning of the 7 variables are as follow: 
id: Weather station ID  
date: Date of observation  
prcp: Precipitation (tenths of mm)  
snow: Snowfall (mm)  
snwd: Snow depth (mm)  
tmax: Maximum temperature (tenths of degrees C)  
tmin: Minimum temperature (tenths of degrees C)  
Observations for temperature, precipitation and snowfall will be converted to proper units for interpretation.  
Each weather station may collect only a subset of these variables, and therefore the resulting dataset contains extensive missing data. Of all observations, 1372743 contain at least one missing value. In addition, 72278 have missing values for all five key variables. There are `r ny_noaa %>% filter(is.na(prcp)) %>% nrow()` (5.62%) missing values for precipitation and `r ny_noaa %>% filter(is.na(tmax)) %>% nrow()` (43.71%) missing values for tmax and `r ny_noaa %>% filter(is.na(tmin)) %>% nrow()` (43.71%) missing values for tmin. Also, there are `r ny_noaa %>% filter(is.na(snow)) %>% nrow()` (14.69%) missing values for snowfall and `r ny_noaa %>% filter(is.na(snwd)) %>% nrow()` (22.80%) missing values for snow depth. A large extent of the data are missing and this might be an issue in the analysis and interpretation of this dataset.

Do or answer questions:

a.Do some data cleaning. Create separate variables for year, month, and day. Ensure observations for temperature, precipitation, and snowfall are given in reasonable units. For snowfall, what are the most commonly observed values? Why?
```{r clean_ny_noaa}
ny_clean = ny_noaa %>% 
  separate(date, into = c("year", "month", "day"), sep = "-") %>% 
  mutate(prcp = prcp / 10, tmin = as.numeric(tmin) / 10, 
         tmax = as.numeric(tmax) / 10) 

ny_clean %>% 
  group_by(snow) %>% 
  summarise(n_observed = n()) %>% 
  arrange(desc(n_observed))
```
I used separate() to create separete variables for year, month and day. Observations for temperature, precipitation, and snowfall are given in reasonable units after cleaning. The unit of precipitation is changed from tenths of mm to mm. The unit of maximum and minimum temperature is changed from tenths of degrees C to degrees C. And tmax and tmin are coverted into numeric variables.
For snowfall, "0" is the most commonly observed value. That is because in most time of the year, there is no snowfall in New York.
